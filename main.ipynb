{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "from src.models import *\n",
    "from src.dataset import DataModule\n",
    "from src.trainers import PhoBERTModel, FastTextLSTMModel\n",
    "\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from lightning.pytorch.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    # for data\n",
    "    root_data_dir = './data'\n",
    "    model_type = 'bert'  # or 'lstm' for FastText-LSTM\n",
    "    batch_size = 16\n",
    "    num_workers = 2\n",
    "    fasttext_embedding = None  # Otherwise specify path to embedding like src/embedding/fasttext_train_dev.model\n",
    "\n",
    "    # for model\n",
    "    model_name = 'BERT-FF-BASE'\n",
    "    # [BERT | LSTM]-[FF | LSTM]-[BASE | LARGE] \n",
    "    # FASTTEXT-LSTM: FastText + LSTM\n",
    "    from_pretrained = True\n",
    "    freeze_backbone = False\n",
    "    drop_out = 0.1\n",
    "    out_channels = 3\n",
    "    vector_size = 300  # For FastText\n",
    "\n",
    "    # for trainer\n",
    "    seed = 42\n",
    "    max_epochs = 100\n",
    "    val_each_epoch = 2\n",
    "    learning_rate = 1e-4\n",
    "    accelarator = \"gpu\"\n",
    "\n",
    "    # TENSORBOARD LOGGING\n",
    "    tensorboard = {\n",
    "        'dir': 'logging',\n",
    "        'name': 'experiment',\n",
    "        'version': 0\n",
    "    }\n",
    "\n",
    "    # ckpt\n",
    "    ckpt_dir = 'logging/experiment/0/ckpt'\n",
    "\n",
    "    # CKPT FOR EVALUATE\n",
    "    test_ckpt = None\n",
    "\n",
    "    # CKPT FOR CONTINUE TRAINING\n",
    "    keep_training_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DataModule(root_data_dir=config.root_data_dir,  \n",
    "                    model_type=config.model_type, \n",
    "                    batch_size=config.batch_size, \n",
    "                    num_workers=config.num_workers, \n",
    "                    fasttext_embedding=config.fasttext_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.setup('fit')\n",
    "loss_weight = dm.train_data.class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.model_name == \"BERT-FF-BASE\":\n",
    "        model = PhoBertFeedForward_base(from_pretrained=config.from_pretrained,\n",
    "                                        freeze_backbone=config.freeze_backbone,\n",
    "                                        drop_out=config.drop_out,\n",
    "                                        out_channels=config.out_channels)\n",
    "elif config.model_name == \"BERT-FF-LARGE\":\n",
    "    model = PhoBertFeedForward_large(from_pretrained=config.from_pretrained,\n",
    "                                    freeze_backbone=config.freeze_backbone,\n",
    "                                    drop_out=config.drop_out,\n",
    "                                    out_channels=config.out_channels)\n",
    "elif config.model_name == \"BERT-LSTM-BASE\":\n",
    "    model = PhoBERTLSTM_base(from_pretrained=config.from_pretrained,\n",
    "                                    freeze_backbone=config.freeze_backbone,\n",
    "                                    drop_out=config.drop_out,\n",
    "                                    out_channels=config.out_channels)\n",
    "elif config.model_name == \"BERT-LSTM-LARGE\":\n",
    "    model = PhoBERTLSTM_large(from_pretrained=config.from_pretrained,\n",
    "                                    freeze_backbone=config.freeze_backbone,\n",
    "                                    drop_out=config.drop_out,\n",
    "                                    out_channels=config.out_channels)\n",
    "elif config.model_name == \"FASTTEXT-LSTM\":\n",
    "    pass\n",
    "else:\n",
    "    raise ValueError(f\"Not support {config.model_name}\")\n",
    "\n",
    "# system configuration\n",
    "if config.model_name.startswith(\"FASTTEXT\"):\n",
    "    system = FastTextLSTMModel(dropout=config.drop_out, \n",
    "                                out_channels=config.out_channels,\n",
    "                                hidden_size=config.vector_size,\n",
    "                                loss_weight=loss_weight)\n",
    "else:\n",
    "    system = PhoBERTModel(model=model, \n",
    "                            out_channels=config.out_channels,\n",
    "                            loss_weight=loss_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(dirpath=config.ckpt_dir, \n",
    "                                      monitor=\"val_loss\", \n",
    "                                      save_top_k=3, mode=\"min\")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\")\n",
    "\n",
    "logger = TensorBoardLogger(save_dir=config.tensorboard['dir'], \n",
    "                        name=config.tensorboard['name'], \n",
    "                        version=config.tensorboard['version'])\n",
    "\n",
    "trainer = Trainer(accelerator=config.accelarator, check_val_every_n_epoch=config.val_each_epoch,\n",
    "                gradient_clip_val=1.0,max_epochs=config.max_epochs,\n",
    "                enable_checkpointing=True, deterministic=True, default_root_dir=config.ckpt_dir,\n",
    "                callbacks=[checkpoint_callback, early_stopping], logger=logger, accumulate_grad_batches=4,log_every_n_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model=system, datamodule=dm, ckpt_path=config.keep_training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model=system, datamodule=dm, ckpt_path=config.test_ckpt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
